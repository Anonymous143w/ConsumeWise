{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anonymous143w/-ConsumeWise-/blob/KrishnaDev/model/consumewise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU8q_mPnn4zl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df= pd.read_csv('/content/nndb_flat.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eImDaAYm5Er",
        "outputId": "88740f4e-5aaf-4611-f654-0313ac66067d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ID', 'FoodGroup', 'ShortDescrip', 'Descrip', 'CommonName', 'MfgName', 'ScientificName', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g', 'Sugar_g', 'Fiber_g', 'VitA_mcg', 'VitB6_mg', 'VitB12_mcg', 'VitC_mg', 'VitE_mg', 'Folate_mcg', 'Niacin_mg', 'Riboflavin_mg', 'Thiamin_mg', 'Calcium_mg', 'Copper_mcg', 'Iron_mg', 'Magnesium_mg', 'Manganese_mg', 'Phosphorus_mg', 'Selenium_mcg', 'Zinc_mg', 'VitA_USRDA', 'VitB6_USRDA', 'VitB12_USRDA', 'VitC_USRDA', 'VitE_USRDA', 'Folate_USRDA', 'Niacin_USRDA', 'Riboflavin_USRDA', 'Thiamin_USRDA', 'Calcium_USRDA', 'Copper_USRDA', 'Magnesium_USRDA', 'Phosphorus_USRDA', 'Selenium_USRDA', 'Zinc_USRDA']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "attribute_titles = pd.read_csv('/content/nndb_flat.csv').columns.tolist()\n",
        "\n",
        "print(attribute_titles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1PAShlYxfyz",
        "outputId": "4ad51d0d-0aa8-4d0e-c0ed-31c9b74fa77f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe5kArnq0SIJ",
        "outputId": "7432732a-935f-4c07-a625-1c9f1f54d993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRHEKeH65Yq4",
        "outputId": "cfb14f7c-8955-4f91-f7b3-b2a8c3395ac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.45.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.20.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhIidoKI1eRP",
        "outputId": "16cdb208-49ec-4801-d878-084ba4e22d73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "UQUZKRuN5LnN",
        "outputId": "f7265b68-d6c5-4d94-9495-2adcb78b7c75"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport os\\nimport pickle\\n# Load the CSV file\\ndf = pd.read_csv(\\'/content/nndb_flat.csv\\')\\n# Initialize the sentence transformer model for embeddings\\nembedder = SentenceTransformer(\\'all-MiniLM-L6-v2\\')\\n\\n# Check if embeddings are already computed and saved\\nif os.path.exists(\\'food_embeddings.pkl\\'):\\n    try: # Try to open the pickle file and load data\\n        with open(\\'food_embeddings.pkl\\', \\'rb\\') as f:\\n            food_embeddings = pickle.load(f)\\n    except EOFError: # Handle the exception if the file is empty\\n        print(\"The pickle file is empty. Recalculating embeddings.\")\\n        # Create embeddings for each food item\\n        food_embeddings = embedder.encode(df[\\'Descrip\\'].tolist(), show_progress_bar=True)\\n        # Save embeddings for future use\\n        with open(\\'food_embeddings.pkl\\', \\'wb\\') as f:\\n            pickle.dump(food_embeddings, f)        \\nelse:\\n    # Create embeddings for each food item\\n    food_embeddings = embedder.encode(df[\\'Descrip\\'].tolist(), show_progress_bar=True)\\n    # Save embeddings for future use\\n    with open(\\'food_embeddings.pkl\\', \\'wb\\') as f:\\n        pickle.dump(food_embeddings, f)\\n\\n#Import the NearestNeighbors class \\nfrom sklearn.neighbors import NearestNeighbors\\n# Initialize Nearest Neighbors model\\nnn = NearestNeighbors(n_neighbors=5, metric=\\'cosine\\')\\nnn.fit(food_embeddings)\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "import os\n",
        "import pickle\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/nndb_flat.csv')\n",
        "# Initialize the sentence transformer model for embeddings\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Check if embeddings are already computed and saved\n",
        "if os.path.exists('food_embeddings.pkl'):\n",
        "    try: # Try to open the pickle file and load data\n",
        "        with open('food_embeddings.pkl', 'rb') as f:\n",
        "            food_embeddings = pickle.load(f)\n",
        "    except EOFError: # Handle the exception if the file is empty\n",
        "        print(\"The pickle file is empty. Recalculating embeddings.\")\n",
        "        # Create embeddings for each food item\n",
        "        food_embeddings = embedder.encode(df['Descrip'].tolist(), show_progress_bar=True)\n",
        "        # Save embeddings for future use\n",
        "        with open('food_embeddings.pkl', 'wb') as f:\n",
        "            pickle.dump(food_embeddings, f)\n",
        "else:\n",
        "    # Create embeddings for each food item\n",
        "    food_embeddings = embedder.encode(df['Descrip'].tolist(), show_progress_bar=True)\n",
        "    # Save embeddings for future use\n",
        "    with open('food_embeddings.pkl', 'wb') as f:\n",
        "        pickle.dump(food_embeddings, f)\n",
        "\n",
        "#Import the NearestNeighbors class\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "# Initialize Nearest Neighbors model\n",
        "nn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
        "nn.fit(food_embeddings)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfzkRxJiDPtZ",
        "outputId": "c6406a92-e835-4f29-be0f-ade40b99f5b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/nndb_flat.csv')\n",
        "# Initialize the sentence transformer model for embeddings\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Create embeddings for each food item\n",
        "food_embeddings = embedder.encode(df['Descrip'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sVembsjfFOK",
        "outputId": "474de4f7-58bd-4742-c511-56acc1798714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XU9Ojorw5u_m"
      },
      "outputs": [],
      "source": [
        "# Initialize the Llama 3.2 model and tokenizer\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH4zrGHhBZfV"
      },
      "outputs": [],
      "source": [
        "def get_relevant_foods(query, top_k=5):\n",
        "    query_embedding = embedder.encode([query])\n",
        "    similarities = cosine_similarity(query_embedding, food_embeddings)[0]\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "    return df.iloc[top_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhSGJEIOBcHh"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, relevant_foods):\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following food based on the query: \"{query}\"\n",
        "\n",
        "    Food information:\n",
        "    {relevant_foods[['Descrip', 'Energy_kcal', 'Protein_g', 'Fat_g', 'Carb_g', 'Sugar_g', 'Fiber_g', 'VitA_USRDA', 'VitC_USRDA', 'Calcium_USRDA', 'Iron_mg']].to_string(index=False)}\n",
        "\n",
        "    Please provide:\n",
        "    1. A health score from 1 to 10 (1 being least healthy, 10 being most healthy)\n",
        "    2. Reasons for the given score\n",
        "    3. Alternative healthier options if applicable\n",
        "\n",
        "    Your response:\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=500, num_return_sequences=1, temperature=0.7)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4sCbBtH50ee"
      },
      "outputs": [],
      "source": [
        "def process_query(query):\n",
        "    relevant_foods = get_relevant_foods(query)\n",
        "    response = generate_response(query, relevant_foods)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yewXtzHV-q75",
        "outputId": "1dc31dd4-67ae-461a-8421-8b20dbdcdf22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    Analyze the following food based on the query: \"Is white bread healthy?\"\n",
            "\n",
            "    Food information:\n",
            "                                                             Descrip  Energy_kcal  Protein_g  Fat_g  Carb_g  Sugar_g  Fiber_g  VitA_USRDA  VitC_USRDA  Calcium_USRDA  Iron_mg\n",
            "                                   Bread, reduced-calorie, white        207.0       8.70   2.50   44.30     4.76      9.7         0.0    0.005556       0.078333     3.19\n",
            "                    Bread, white, commercially prepared, toasted        293.0       9.00   4.00   54.40     4.74      2.5         0.0    0.000000       0.099167     3.33\n",
            "                                       Bread, wheat, white wheat        238.0      10.66   2.15   43.91     5.00      9.2         0.0    0.000000       0.570000     4.89\n",
            "Bread, white, commercially prepared (includes soft bread crumbs)        266.0       8.85   3.33   49.42     5.67      2.7         0.0    0.000000       0.120000     3.61\n",
            "                             Wheat flour, white, bread, enriched        361.0      11.98   1.66   72.53     0.31      2.4         0.0    0.000000       0.012500     4.41\n",
            "\n",
            "    Please provide:\n",
            "    1. A health score from 1 to 10 (1 being least healthy, 10 being most healthy)\n",
            "    2. Reasons for the given score\n",
            "    3. Alternative healthier options if applicable\n",
            "\n",
            "    Your response:\n",
            "     The health score is 3.6\n",
            "     Reason: 1. Low amount of fiber\n",
            "     Alternative healthier options: 1. Whole grain bread\n",
            "     Reason: 2. Low amount of fiber\n",
            "     Alternative healthier options: 2. Multigrain bread\n",
            "     Reason: 3. Low amount of fiber\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "user_query = \"Is white bread healthy?\"\n",
        "result = process_query(user_query)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UryGixh-sU1"
      },
      "outputs": [],
      "source": [
        "# Save the sentence transformer model\n",
        "embedder.save('sentence_transformer_model')\n",
        "\n",
        "# Save the tokenizer and Llama model\n",
        "tokenizer.save_pretrained('llama_tokenizer')\n",
        "model.save_pretrained('llama_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcsgqmZbhcC_"
      },
      "outputs": [],
      "source": [
        "# Load the sentence transformer model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedder = SentenceTransformer('sentence_transformer_model')\n",
        "\n",
        "# Load the tokenizer and Llama model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer = AutoTokenizer.from_pretrained('llama_tokenizer')\n",
        "model = AutoModelForCausalLM.from_pretrained('llama_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7SGsXkajpaA",
        "outputId": "d2ee56ee-3013-423f-c1c9-c1a97d573522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1KPNX6wtjsA0",
        "outputId": "728910ba-dd73-4f36-ae8f-cca99048dc7b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/my_models/sentence_transformer_model'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.copytree('sentence_transformer_model', '/content/drive/MyDrive/my_models/sentence_transformer_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tw8QtRBTj2Ki",
        "outputId": "f114dad1-f2d5-4f59-bf37-771b7cc4d81c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/my_models/llama_model'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.copytree('llama_model', '/content/drive/MyDrive/my_models/llama_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UJ8Pd-s3kHsi",
        "outputId": "7408c100-0dc4-4d41-b5f5-d633d16b38b3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/my_models/llama_tokenizer'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.copytree('llama_tokenizer', '/content/drive/MyDrive/my_models/llama_tokenizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4zMr8iGkkRC",
        "outputId": "a661a438-6753-41e0-9b90-10b90617c7be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "\n",
        "# Load the saved models and other necessary libraries\n",
        "# ...\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/analyze', methods=['POST'])\n",
        "def analyze_food():\n",
        "    data = request.get_json()\n",
        "    query = data.get('query')\n",
        "    if query:\n",
        "        response = process_query(query)\n",
        "        return jsonify({'response': response})\n",
        "    else:\n",
        "        return jsonify({'error': 'No query provided'}), 400\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True, host='0.0.0.0', port=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLG_8L0EksZu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}